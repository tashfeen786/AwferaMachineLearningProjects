{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Probability Concepts with Real-World Examples\n",
        "\n",
        "## Real-World Applications of Probability\n",
        "\n",
        "### Autocorrect Systems\n",
        "- **How it works**:\n",
        "  - When you type \"recieve\", it suggests \"receive\"\n",
        "  - Uses word frequency statistics (unigram/bigram probabilities)\n",
        "\n",
        "\n",
        "### Spam Filters\n",
        "- **How it works**:\n",
        "  - Flags messages like \"Win 1M!\" as spam\n",
        "  - Uses Bayesian probability to update beliefs\n",
        "\n",
        "\n",
        "## Probability Terminology Explained\n",
        "\n",
        "### Key Concepts with Email Examples\n",
        "\n",
        "#### Random Experiment\n",
        "- **Definition**: A process that produces an outcome\n",
        "- **Email example**: Classifying an email as spam/ham\n",
        "- **Other examples**: Rolling a die, flipping a coin\n",
        "\n",
        "#### Trial\n",
        "- **Definition**: One execution of a random experiment\n",
        "- **Email example**: Processing one email message\n",
        "- **Other examples**: One coin flip, one die roll\n",
        "\n",
        "#### Outcome\n",
        "- **Definition**: The result of a trial\n",
        "- **Email example**: \"spam\" or \"not spam\" classification\n",
        "- **Other examples**: \"heads\", \"tails\", die face number\n",
        "\n",
        "#### Sample Space\n",
        "- **Definition**: All possible outcomes\n",
        "- **Email example**: All possible email texts and classifications\n",
        "- **Notation**: S = {\"spam\", \"not spam\"}\n",
        "\n",
        "\n",
        "### Corpus in Natural Language Processing\n",
        "\n",
        "#### Definition\n",
        "A **corpus** (plural: corpora) is a large, structured collection of texts used for linguistic analysis and training language models."
      ],
      "metadata": {
        "id": "w94_MfiPIiko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Next-Word Prediction"
      ],
      "metadata": {
        "id": "mmrVlUyrhshg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Build Your Own Predictor\n",
        "\n",
        "### Challenge\n",
        "Design an algorithm that predicts the next word given typing history.\n",
        "\n",
        "**Example Input/Output:**\n",
        "- Input: \"I want to\"\n",
        "- Output:\n",
        "  - \"go\" (45% probability)\n",
        "  - \"eat\" (30% probability)\n",
        "  - \"sleep\" (25% probability)\n",
        "\n",
        "## Next-Word Prediction Pipeline\n",
        "\n",
        "### Step-by-Step Process\n",
        "1. **Analyze current phrase**  \n",
        "   Example: \"How are\"\n",
        "\n",
        "2. **Generate likely candidates**  \n",
        "   Potential next words: \"you\", \"doing\", \"we\"\n",
        "\n",
        "3. **Rank by conditional probability**  \n",
        "   Calculate P(word | context) for each candidate\n",
        "\n",
        "\n",
        "# Conceptual implementation\n",
        "context = \"How are\"\n",
        "candidates = [\"you\", \"doing\", \"we\"]\n",
        "\n",
        "# Calculate probabilities\n",
        "\n",
        "Probabilities\n",
        "\n",
        "    \"you\": 0.72,    # P(\"you\" | \"How are\")\n",
        "\n",
        "    \"doing\": 0.18,   # P(\"doing\" | \"How are\")\n",
        "\n",
        "    \"we\": 0.10       # P(\"we\" | \"How are\")\n"
      ],
      "metadata": {
        "id": "iiFrcc1OMHin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram Language Models\n",
        "\n",
        "## Markov Assumption\n",
        "The probability of a word depends only on the previous **k** words:\n",
        "\n",
        "P(w_n | w_1...w_{n-1}) ≈ P(w_n | w_{n-k}...w_{n-1})\n",
        "\n",
        "\n",
        "## Unigram Model\n",
        "**Assumption**: Words are completely independent  \n",
        "**Probability**:\n",
        "P(w) = count(w) / total_words\n",
        "\n",
        "**Example**: \"I love you\"\n",
        "\n",
        "P(\"I love you\") = P(\"I\") × P(\"love\") × P(\"you\")\n",
        "\n",
        "\n",
        "## Bigram Model\n",
        "**Assumption**: Word depends on 1 previous word  \n",
        "**Probability**:\n",
        "P(w_n | w_{n-1}) = count(w_{n-1} w_n) / count(w_{n-1})\n",
        "\n",
        "**Example**: \"I love you\"\n",
        "P(\"I love you\") = P(\"I\") × P(\"love\"|\"I\") × P(\"you\"|\"love\")\n",
        "\n",
        "## Trigram Model\n",
        "**Assumption**: Word depends on 2 previous words  \n",
        "**Probability**:\n",
        "P(w_n | w_{n-2}, w_{n-1}) = count(w_{n-2} w_{n-1} w_n) / count(w_{n-2} w_{n-1})\n",
        "\n",
        "**Example**: \"I love you\"\n",
        "P(\"I love you\") = P(\"I\") × P(\"love\"|\"I\") × P(\"you\"|\"I love\")\n",
        "\n",
        "## Comparison Table\n",
        "\n",
        "| Model    | Dependency          | Example Probability            |\n",
        "|----------|---------------------|--------------------------------|\n",
        "| Unigram  | None                | P(\"love\")                     |\n",
        "| Bigram   | 1 previous word     | P(\"you\"/\"love\")              |\n",
        "| Trigram  | 2 previous words    | P(\"you\"/\"I love\")            |\n",
        "\n"
      ],
      "metadata": {
        "id": "meAa3cmm7Vc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Dataset\n",
        "Using Shakespeare's sonnets (public domain):\n",
        "```python\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "2vQjbt00dV1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading & Preprocessing"
      ],
      "metadata": {
        "id": "BjoBc76hgsf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filepath):\n",
        "    \"\"\"Load text file and return as string\"\"\"\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Preprocess text by:\n",
        "    1. Converting to lowercase\n",
        "    2. Removing special characters\n",
        "    3. Splitting into tokens (words)\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "\n",
        "    text = re.sub(r\"[^a-z0-9'\\s]\", \"\", text)\n",
        "\n",
        "    # Split into tokens (words)\n",
        "    tokens = text.split()\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# TEST\n",
        "text = load_data(\"input.txt\")\n",
        "tokens = preprocess(text)\n",
        "print(\"First 50 tokens:\", tokens[:50])"
      ],
      "metadata": {
        "id": "MRQNYUD6fDxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Implementation"
      ],
      "metadata": {
        "id": "n1OVBvXufmJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ngram_model(tokens, n=2):\n",
        "    \"\"\"\n",
        "    Build n-gram model from tokens\n",
        "    Example: For n=2 (bigram), returns {\"i\": {\"am\": 5, \"have\": 3}}\n",
        "    \"\"\"\n",
        "    model = {}\n",
        "\n",
        "    # TODO: Implement this function\n",
        "    # 1. Loop through tokens with sliding window of size n\n",
        "    # 2. For each window, update model counts\n",
        "    # Hint: Use zip(*[tokens[i:] for i in range(n)]) for sliding window\n",
        "\n",
        "    return model\n",
        "\n",
        "def predict_next_word(model, context, k=3):\n",
        "    \"\"\"\n",
        "    Predict next words given context\n",
        "    Returns: list of (word, probability) tuples\n",
        "    \"\"\"\n",
        "    # TODO: Implement this function\n",
        "    # 1. Get possible next words from model[context]\n",
        "    # 2. Calculate probabilities (count/total)\n",
        "    # 3. Return top k most probable words\n",
        "\n",
        "    return []"
      ],
      "metadata": {
        "id": "wPWcaec8fmmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "PN_daDF4hOL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model = build_ngram_model(tokens, n=2)\n",
        "print(\"Words after 'the':\", predict_next_word(bigram_model, \"the\"))"
      ],
      "metadata": {
        "id": "dpDN8OPMhQGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: Spam/Ham Classification"
      ],
      "metadata": {
        "id": "6bM0CwKBleEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Spam vs Ham Examples\n",
        "\n",
        "<div style=\"display: flex;\">\n",
        "<div style=\"flex: 50%; padding: 10px;\">\n",
        "\n",
        "### Spam Examples\n",
        "- \"Claim your free \\$1M\"\n",
        "- \"You won an iPhone!\"\n",
        "- \"Limited time offer!\"\n",
        "- \"Click here to claim your prize\"\n",
        "\n",
        "</div>\n",
        "<div style=\"flex: 50%; padding: 10px;\">\n",
        "\n",
        "### Ham Examples\n",
        "- \"Meeting at 3 PM\"\n",
        "- \"Project update attached\"\n",
        "- \"Let's discuss the proposal\"\n",
        "- \"Quarterly report is ready\"\n",
        "\n",
        "</div>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "05pMj5bCdEZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Prior Probability (P(c))\n",
        "**What it represents:**  \n",
        "The baseline probability of a class before examining the message content.\n",
        "\n",
        "**Example Calculation:**  \n",
        "\n",
        "#### In 100 emails:\n",
        "spam_count = 30\n",
        "\n",
        "ham_count = 70\n",
        "\n",
        "P_spam = spam_count/100  # 0.30\n",
        "\n",
        "P_ham = ham_count/100    # 0.70"
      ],
      "metadata": {
        "id": "8fXL2dhUdIbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Likelihood (P(w|c))\n",
        "\n",
        "**Definition:**  \n",
        "The probability of observing a specific word given a particular class (spam/ham). This measures how characteristic a word is for a certain class.\n",
        "\n",
        "\n",
        "#### Spam class: 25 occurrences out of 1000 total words\n",
        "P_free_spam = 25/1000  # 0.025\n",
        "\n",
        "#### Ham class: 2 occurrences out of 3000 total words\n",
        "P_free_ham = 2/3000    # ≈0.00067\n",
        "\n",
        "### 3. Posterior Probability\n",
        "\n",
        "**Definition:**\n",
        "The posterior probability **P(c|msg)** represents:\n",
        "- The probability that a given message belongs to class *c* (spam/ham)\n",
        "- The key quantity we compute to make classification decisions"
      ],
      "metadata": {
        "id": "X2n9eH38dPAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bayesian Classification\n",
        "\n",
        "### Decision Rule\n",
        "The classifier selects the class with the highest posterior probability:\n",
        "\n",
        "Where:\n",
        "- `P(c|msg)` = Posterior probability (what we want to calculate)\n",
        "- `P(msg|c)` = Likelihood of the message given the class\n",
        "- `P(c)` = Prior probability of the class\n",
        "\n",
        "### Practical Example: \"free prize\" message\n",
        "\n",
        "**Given Probabilities:**\n",
        "\n",
        "### Word probabilities\n",
        "P_free_spam = 0.025\n",
        "\n",
        "P_free_ham = 0.00067\n",
        "\n",
        "P_prize_spam = 0.015\n",
        "\n",
        "P_prize_ham = 0.001\n",
        "\n",
        "\n",
        "\n",
        "### Class priors\n",
        "P_spam = 0.3\n",
        "\n",
        "P_ham = 0.7"
      ],
      "metadata": {
        "id": "vUzlLGbmdWqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculation Steps\n",
        "#### Compute Spam Probability:\n",
        "P_msg_spam = P_free_spam * P_prize_spam * P_spam\n",
        "           = 0.025 × 0.015 × 0.3\n",
        "           ≈ 0.0001125\n",
        "\n",
        "####Compute Ham Probability:\n",
        "\n",
        "P_msg_ham = P_free_ham * P_prize_ham * P_ham\n",
        "          = 0.00067 × 0.001 × 0.7\n",
        "          ≈ 0.000000469\n",
        "\n",
        "####Comparison:\n",
        "0.0001125 (Spam) > 0.000000469 (Ham)"
      ],
      "metadata": {
        "id": "tYTTohG0NtWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Laplace Smoothing\n",
        "**Problem**:  \n",
        "P(\"xyz\"|Spam) = 0 if \"xyz\" never appeared in training spam\n",
        "\n",
        "**Solution**:  \n",
        "$$ P_{\\text{smooth}}(w|c) = \\frac{\\text{count}(w,c) + 1}{\\text{count}(c) + V} $$\n",
        "\n",
        "*Example*:  \n",
        "- V = 10,000 words  \n",
        "- count(\"prize\", Spam) = 0  \n",
        "- count(Spam) = 1000  \n",
        "$$ P_{\\text{smooth}}(\"prize\"|Spam) = \\frac{0+1}{1000+10000} \\approx 0.00009 $$\n",
        "\n"
      ],
      "metadata": {
        "id": "JF8gBM4RbBaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Words in Text Processing\n",
        "\n",
        "**Definition:** Common words (e.g., \"the\", \"a\", \"and\") that appear frequently but carry minimal meaning.\n",
        "\n",
        "**Why Remove Them?**\n",
        "1. **Meaning Preservation:**  \n",
        "   \"The quick brown fox\" → \"quick brown fox\" (keeps core meaning)\n",
        "   \n",
        "2. **Improved Analysis:**  \n",
        "   - Without: P(\"the\"|spam)=0.101 vs P(\"the\"|ham)=0.098 (useless)  \n",
        "   - With: P(\"free\"|spam)=0.025 vs P(\"free\"|ham)=0.00067 (discriminative)\n",
        "\n",
        "3. **Efficiency:**  \n",
        "   Reduces vocabulary size by 30-40% (1000 words → 600 words)\n",
        "\n",
        "**Example: Spam Detection**\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "### With stop words\n",
        "text = [\"You have won a free prize\"]\n",
        "vectorizer = CountVectorizer()\n",
        "print(\"With stop words:\", vectorizer.fit_transform(text).toarray())\n",
        "### Output: [[1 1 1 1 1]] (for \"you\", \"have\", \"won\", \"free\", \"prize\")\n",
        "\n",
        "### Without stop words\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "print(\"Without stop words:\", vectorizer.fit_transform(text).toarray())\n",
        "### Output: [[1 1 1]] (for \"won\", \"free\", \"prize\")"
      ],
      "metadata": {
        "id": "x5WfNQ7Wa4fb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Loading"
      ],
      "metadata": {
        "id": "s1BUOSqZtiam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
        "!unzip smsspamcollection.zip"
      ],
      "metadata": {
        "id": "3RAs_lxNmAwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sms_data(filepath):\n",
        "    \"\"\"Load SMS spam data from file.\n",
        "    Returns: (list of messages, list of labels)\"\"\"\n",
        "    messages = []\n",
        "    labels = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) == 2:\n",
        "                labels.append(parts[0])\n",
        "                messages.append(parts[1])\n",
        "    return messages, labels\n",
        "\n",
        "def preprocess_text(text, remove_stopwords=True):\n",
        "    \"\"\"Clean text with optional stopword removal\"\"\"\n",
        "    import re\n",
        "\n",
        "    # Basic cleaning\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    if remove_stopwords:\n",
        "        stop_words = {\n",
        "            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
        "            'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him',\n",
        "            'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its',\n",
        "            'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
        "            'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n",
        "            'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
        "            'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
        "            'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',\n",
        "            'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
        "            'against', 'between', 'into', 'through', 'during', 'before',\n",
        "            'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
        "            'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
        "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
        "            'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
        "            'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same',\n",
        "            'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just',\n",
        "            'don', 'should', 'now'\n",
        "        }\n",
        "\n",
        "        words = text.split()\n",
        "        filtered_words = [word for word in words if word not in stop_words]\n",
        "        text = ' '.join(filtered_words)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test with stopword removal\n",
        "messages, labels = load_sms_data(\"SMSSpamCollection\")\n",
        "print(\"Original:\", messages[0])\n",
        "print(\"Processed:\", preprocess_text(messages[0]))\n",
        "print(\"\\nWithout stopwords:\", preprocess_text(messages[0], remove_stopwords=True))"
      ],
      "metadata": {
        "id": "tV5UTqbYmW7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculate Prior Probabilities"
      ],
      "metadata": {
        "id": "PEz8cwvrrFQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_priors(labels):\n",
        "    # TODO: Compute P(spam) = (#spam messages)/(total messages)\n",
        "    # TODO: Compute P(ham) similarly\n",
        "    return prior_prob"
      ],
      "metadata": {
        "id": "CnMKvAvepFlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Count Words per Class"
      ],
      "metadata": {
        "id": "zyJZPtSnrImM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(messages, labels):\n",
        "    # TODO: Populate word_counts[class][word] = frequency\n",
        "    # Hint: Use preprocess_text() from earlier\n",
        "    # Don't forget to update vocab!\n",
        "    return word_counts, vocab"
      ],
      "metadata": {
        "id": "6Ej5JOXfpJZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compute Word Probabilities (with Smoothing)"
      ],
      "metadata": {
        "id": "2PQwY3WkrPoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_word_probs(k=1):\n",
        "    # TODO: For each word in vocab:\n",
        "    # P(word|spam) = (count in spam + k)/(total spam words + k*V)\n",
        "    # V = vocabulary size (len(vocab))\n",
        "    return word_probs"
      ],
      "metadata": {
        "id": "gx1dmadApKNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predict Spam Probability"
      ],
      "metadata": {
        "id": "nBaetzsrrbYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(message, prior_prob, word_probs):\n",
        "    # TODO:\n",
        "    # 1. Preprocess message and split into words\n",
        "    # 2. Initialize log_prob_spam = log(P(spam))\n",
        "    # 3. For each word:\n",
        "    #    - if word in vocab: add log(P(word|spam))\n",
        "    # 4. Convert log probabilities back using exp()\n",
        "    # 5. Return P(spam|message) = spam_prob/(spam_prob + ham_prob)\n",
        "    return probability"
      ],
      "metadata": {
        "id": "SbCxkksUpT7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test calculate_priors()\n",
        "labels = ['spam', 'ham']\n",
        "print(calculate_priors(labels))  # Should return {'spam': 0.33, 'ham': 0.67}\n",
        "\n",
        "# Test count_words()\n",
        "messages = [\"free prize\", \"meeting today\"]\n",
        "word_counts, vocab = count_words(messages, labels)\n",
        "print(word_counts['spam']['free'])  # Should be 1\n",
        "print(len(vocab))  # Should be 3 (free, prize, meeting, today)\n",
        "\n",
        "# Test calculate_word_probs()\n",
        "word_probs = calculate_word_probs(k=1)\n",
        "print(word_probs['spam']['free'])  # Should be (1+1)/(2+1*4) = 0.33"
      ],
      "metadata": {
        "id": "4bSCRCmWqPcn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}