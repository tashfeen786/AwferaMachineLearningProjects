{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w94_MfiPIiko"
   },
   "source": [
    "# Probability Concepts with Real-World Examples\n",
    "\n",
    "## Real-World Applications of Probability\n",
    "\n",
    "### Autocorrect Systems\n",
    "- **How it works**:\n",
    "  - When you type \"recieve\", it suggests \"receive\"\n",
    "  - Uses word frequency statistics (unigram/bigram probabilities)\n",
    "\n",
    "\n",
    "### Spam Filters\n",
    "- **How it works**:\n",
    "  - Flags messages like \"Win 1M!\" as spam\n",
    "  - Uses Bayesian probability to update beliefs\n",
    "\n",
    "\n",
    "## Probability Terminology Explained\n",
    "\n",
    "### Key Concepts with Email Examples\n",
    "\n",
    "#### Random Experiment\n",
    "- **Definition**: A process that produces an outcome\n",
    "- **Email example**: Classifying an email as spam/ham\n",
    "- **Other examples**: Rolling a die, flipping a coin\n",
    "\n",
    "#### Trial\n",
    "- **Definition**: One execution of a random experiment\n",
    "- **Email example**: Processing one email message\n",
    "- **Other examples**: One coin flip, one die roll\n",
    "\n",
    "#### Outcome\n",
    "- **Definition**: The result of a trial\n",
    "- **Email example**: \"spam\" or \"not spam\" classification\n",
    "- **Other examples**: \"heads\", \"tails\", die face number\n",
    "\n",
    "#### Sample Space\n",
    "- **Definition**: All possible outcomes\n",
    "- **Email example**: All possible email texts and classifications\n",
    "- **Notation**: S = {\"spam\", \"not spam\"}\n",
    "\n",
    "\n",
    "### Corpus in Natural Language Processing\n",
    "\n",
    "#### Definition\n",
    "A **corpus** (plural: corpora) is a large, structured collection of texts used for linguistic analysis and training language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmrVlUyrhshg"
   },
   "source": [
    "# Exercise 1: Next-Word Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiFrcc1OMHin"
   },
   "source": [
    "\n",
    "\n",
    "## Build Your Own Predictor\n",
    "\n",
    "### Challenge\n",
    "Design an algorithm that predicts the next word given typing history.\n",
    "\n",
    "**Example Input/Output:**\n",
    "- Input: \"I want to\"\n",
    "- Output:\n",
    "  - \"go\" (45% probability)\n",
    "  - \"eat\" (30% probability)\n",
    "  - \"sleep\" (25% probability)\n",
    "\n",
    "## Next-Word Prediction Pipeline\n",
    "\n",
    "### Step-by-Step Process\n",
    "1. **Analyze current phrase**  \n",
    "   Example: \"How are\"\n",
    "\n",
    "2. **Generate likely candidates**  \n",
    "   Potential next words: \"you\", \"doing\", \"we\"\n",
    "\n",
    "3. **Rank by conditional probability**  \n",
    "   Calculate P(word | context) for each candidate\n",
    "\n",
    "\n",
    "# Conceptual implementation\n",
    "context = \"How are\"\n",
    "candidates = [\"you\", \"doing\", \"we\"]\n",
    "\n",
    "# Calculate probabilities\n",
    "\n",
    "Probabilities\n",
    "\n",
    "    \"you\": 0.72,    # P(\"you\" | \"How are\")\n",
    "\n",
    "    \"doing\": 0.18,   # P(\"doing\" | \"How are\")\n",
    "\n",
    "    \"we\": 0.10       # P(\"we\" | \"How are\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meAa3cmm7Vc5"
   },
   "source": [
    "# N-gram Language Models\n",
    "\n",
    "## Markov Assumption\n",
    "The probability of a word depends only on the previous **k** words:\n",
    "\n",
    "P(w_n | w_1...w_{n-1}) ≈ P(w_n | w_{n-k}...w_{n-1})\n",
    "\n",
    "\n",
    "## Unigram Model\n",
    "**Assumption**: Words are completely independent  \n",
    "**Probability**:\n",
    "P(w) = count(w) / total_words\n",
    "\n",
    "**Example**: \"I love you\"\n",
    "\n",
    "P(\"I love you\") = P(\"I\") × P(\"love\") × P(\"you\")\n",
    "\n",
    "\n",
    "## Bigram Model\n",
    "**Assumption**: Word depends on 1 previous word  \n",
    "**Probability**:\n",
    "P(w_n | w_{n-1}) = count(w_{n-1} w_n) / count(w_{n-1})\n",
    "\n",
    "**Example**: \"I love you\"\n",
    "P(\"I love you\") = P(\"I\") × P(\"love\"|\"I\") × P(\"you\"|\"love\")\n",
    "\n",
    "## Trigram Model\n",
    "**Assumption**: Word depends on 2 previous words  \n",
    "**Probability**:\n",
    "P(w_n | w_{n-2}, w_{n-1}) = count(w_{n-2} w_{n-1} w_n) / count(w_{n-2} w_{n-1})\n",
    "\n",
    "**Example**: \"I love you\"\n",
    "P(\"I love you\") = P(\"I\") × P(\"love\"|\"I\") × P(\"you\"|\"I love\")\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Model    | Dependency          | Example Probability            |\n",
    "|----------|---------------------|--------------------------------|\n",
    "| Unigram  | None                | P(\"love\")                     |\n",
    "| Bigram   | 1 previous word     | P(\"you\"/\"love\")              |\n",
    "| Trigram  | 2 previous words    | P(\"you\"/\"I love\")            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vQjbt00dV1R"
   },
   "source": [
    "\n",
    "\n",
    "### Dataset\n",
    "Using Shakespeare's sonnets (public domain):\n",
    "```python\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjoBc76hgsf3"
   },
   "source": [
    "#Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRQNYUD6fDxu"
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"Load text file and return as string\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing special characters\n",
    "    3. Splitting into tokens (words)\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "\n",
    "    text = re.sub(r\"[^a-z0-9'\\s]\", \"\", text)\n",
    "\n",
    "    # Split into tokens (words)\n",
    "    tokens = text.split()\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# TEST\n",
    "text = load_data(\"input.txt\")\n",
    "tokens = preprocess(text)\n",
    "print(\"First 50 tokens:\", tokens[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1OVBvXufmJe"
   },
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPWcaec8fmmX"
   },
   "outputs": [],
   "source": [
    "def build_ngram_model(tokens, n=2):\n",
    "    \"\"\"\n",
    "    Build n-gram model from tokens\n",
    "    Example: For n=2 (bigram), returns {\"i\": {\"am\": 5, \"have\": 3}}\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "\n",
    "    # TODO: Implement this function\n",
    "    # 1. Loop through tokens with sliding window of size n\n",
    "    # 2. For each window, update model counts\n",
    "    # Hint: Use zip(*[tokens[i:] for i in range(n)]) for sliding window\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_next_word(model, context, k=3):\n",
    "    \"\"\"\n",
    "    Predict next words given context\n",
    "    Returns: list of (word, probability) tuples\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # 1. Get possible next words from model[context]\n",
    "    # 2. Calculate probabilities (count/total)\n",
    "    # 3. Return top k most probable words\n",
    "\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN_daDF4hOL2"
   },
   "source": [
    "#Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpDN8OPMhQGk"
   },
   "outputs": [],
   "source": [
    "bigram_model = build_ngram_model(tokens, n=2)\n",
    "print(\"Words after 'the':\", predict_next_word(bigram_model, \"the\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bM0CwKBleEW"
   },
   "source": [
    "# Exercise 2: Spam/Ham Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05pMj5bCdEZr"
   },
   "source": [
    "\n",
    "\n",
    "## Spam vs Ham Examples\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "<div style=\"flex: 50%; padding: 10px;\">\n",
    "\n",
    "### Spam Examples\n",
    "- \"Claim your free \\$1M\"\n",
    "- \"You won an iPhone!\"\n",
    "- \"Limited time offer!\"\n",
    "- \"Click here to claim your prize\"\n",
    "\n",
    "</div>\n",
    "<div style=\"flex: 50%; padding: 10px;\">\n",
    "\n",
    "### Ham Examples\n",
    "- \"Meeting at 3 PM\"\n",
    "- \"Project update attached\"\n",
    "- \"Let's discuss the proposal\"\n",
    "- \"Quarterly report is ready\"\n",
    "\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fXL2dhUdIbt"
   },
   "source": [
    "### 1. Prior Probability (P(c))\n",
    "**What it represents:**  \n",
    "The baseline probability of a class before examining the message content.\n",
    "\n",
    "**Example Calculation:**  \n",
    "\n",
    "#### In 100 emails:\n",
    "spam_count = 30\n",
    "\n",
    "ham_count = 70\n",
    "\n",
    "P_spam = spam_count/100  # 0.30\n",
    "\n",
    "P_ham = ham_count/100    # 0.70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2n9eH38dPAP"
   },
   "source": [
    "### 2. Likelihood (P(w|c))\n",
    "\n",
    "**Definition:**  \n",
    "The probability of observing a specific word given a particular class (spam/ham). This measures how characteristic a word is for a certain class.\n",
    "\n",
    "\n",
    "#### Spam class: 25 occurrences out of 1000 total words\n",
    "P_free_spam = 25/1000  # 0.025\n",
    "\n",
    "#### Ham class: 2 occurrences out of 3000 total words\n",
    "P_free_ham = 2/3000    # ≈0.00067\n",
    "\n",
    "### 3. Posterior Probability\n",
    "\n",
    "**Definition:**\n",
    "The posterior probability **P(c|msg)** represents:\n",
    "- The probability that a given message belongs to class *c* (spam/ham)\n",
    "- The key quantity we compute to make classification decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUzlLGbmdWqK"
   },
   "source": [
    "### Bayesian Classification\n",
    "\n",
    "### Decision Rule\n",
    "The classifier selects the class with the highest posterior probability:\n",
    "\n",
    "Where:\n",
    "- `P(c|msg)` = Posterior probability (what we want to calculate)\n",
    "- `P(msg|c)` = Likelihood of the message given the class\n",
    "- `P(c)` = Prior probability of the class\n",
    "\n",
    "### Practical Example: \"free prize\" message\n",
    "\n",
    "**Given Probabilities:**\n",
    "\n",
    "### Word probabilities\n",
    "P_free_spam = 0.025\n",
    "\n",
    "P_free_ham = 0.00067\n",
    "\n",
    "P_prize_spam = 0.015\n",
    "\n",
    "P_prize_ham = 0.001\n",
    "\n",
    "\n",
    "\n",
    "### Class priors\n",
    "P_spam = 0.3\n",
    "\n",
    "P_ham = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYTTohG0NtWK"
   },
   "source": [
    "### Calculation Steps\n",
    "#### Compute Spam Probability:\n",
    "P_msg_spam = P_free_spam * P_prize_spam * P_spam\n",
    "           = 0.025 × 0.015 × 0.3\n",
    "           ≈ 0.0001125\n",
    "\n",
    "####Compute Ham Probability:\n",
    "\n",
    "P_msg_ham = P_free_ham * P_prize_ham * P_ham\n",
    "          = 0.00067 × 0.001 × 0.7\n",
    "          ≈ 0.000000469\n",
    "\n",
    "####Comparison:\n",
    "0.0001125 (Spam) > 0.000000469 (Ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JF8gBM4RbBaW"
   },
   "source": [
    "##Laplace Smoothing\n",
    "**Problem**:  \n",
    "P(\"xyz\"|Spam) = 0 if \"xyz\" never appeared in training spam\n",
    "\n",
    "**Solution**:  \n",
    "$$ P_{\\text{smooth}}(w|c) = \\frac{\\text{count}(w,c) + 1}{\\text{count}(c) + V} $$\n",
    "\n",
    "*Example*:  \n",
    "- V = 10,000 words  \n",
    "- count(\"prize\", Spam) = 0  \n",
    "- count(Spam) = 1000  \n",
    "$$ P_{\\text{smooth}}(\"prize\"|Spam) = \\frac{0+1}{1000+10000} \\approx 0.00009 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5WfNQ7Wa4fb"
   },
   "source": [
    "## Stop Words in Text Processing\n",
    "\n",
    "**Definition:** Common words (e.g., \"the\", \"a\", \"and\") that appear frequently but carry minimal meaning.\n",
    "\n",
    "**Why Remove Them?**\n",
    "1. **Meaning Preservation:**  \n",
    "   \"The quick brown fox\" → \"quick brown fox\" (keeps core meaning)\n",
    "   \n",
    "2. **Improved Analysis:**  \n",
    "   - Without: P(\"the\"|spam)=0.101 vs P(\"the\"|ham)=0.098 (useless)  \n",
    "   - With: P(\"free\"|spam)=0.025 vs P(\"free\"|ham)=0.00067 (discriminative)\n",
    "\n",
    "3. **Efficiency:**  \n",
    "   Reduces vocabulary size by 30-40% (1000 words → 600 words)\n",
    "\n",
    "**Example: Spam Detection**\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "### With stop words\n",
    "text = [\"You have won a free prize\"]\n",
    "vectorizer = CountVectorizer()\n",
    "print(\"With stop words:\", vectorizer.fit_transform(text).toarray())\n",
    "### Output: [[1 1 1 1 1]] (for \"you\", \"have\", \"won\", \"free\", \"prize\")\n",
    "\n",
    "### Without stop words\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "print(\"Without stop words:\", vectorizer.fit_transform(text).toarray())\n",
    "### Output: [[1 1 1]] (for \"won\", \"free\", \"prize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1BUOSqZtiam"
   },
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RAs_lxNmAwA"
   },
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "!unzip smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tV5UTqbYmW7Z"
   },
   "outputs": [],
   "source": [
    "def load_sms_data(filepath):\n",
    "    \"\"\"Load SMS spam data from file.\n",
    "    Returns: (list of messages, list of labels)\"\"\"\n",
    "    messages = []\n",
    "    labels = []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                labels.append(parts[0])\n",
    "                messages.append(parts[1])\n",
    "    return messages, labels\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=True):\n",
    "    \"\"\"Clean text with optional stopword removal\"\"\"\n",
    "    import re\n",
    "\n",
    "    # Basic cleaning\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stop_words = {\n",
    "            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
    "            'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him',\n",
    "            'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its',\n",
    "            'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "            'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n",
    "            'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "            'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
    "            'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',\n",
    "            'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    "            'against', 'between', 'into', 'through', 'during', 'before',\n",
    "            'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "            'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
    "            'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "            'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same',\n",
    "            'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just',\n",
    "            'don', 'should', 'now'\n",
    "        }\n",
    "\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        text = ' '.join(filtered_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Test with stopword removal\n",
    "messages, labels = load_sms_data(\"SMSSpamCollection\")\n",
    "print(\"Original:\", messages[0])\n",
    "print(\"Processed:\", preprocess_text(messages[0]))\n",
    "print(\"\\nWithout stopwords:\", preprocess_text(messages[0], remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEz8cwvrrFQi"
   },
   "source": [
    "#Calculate Prior Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnMKvAvepFlV"
   },
   "outputs": [],
   "source": [
    "def calculate_priors(labels):\n",
    "    # TODO: Compute P(spam) = (#spam messages)/(total messages)\n",
    "    # TODO: Compute P(ham) similarly\n",
    "    return prior_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyJZPtSnrImM"
   },
   "source": [
    "#Count Words per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ej5JOXfpJZn"
   },
   "outputs": [],
   "source": [
    "def count_words(messages, labels):\n",
    "    # TODO: Populate word_counts[class][word] = frequency\n",
    "    # Hint: Use preprocess_text() from earlier\n",
    "    # Don't forget to update vocab!\n",
    "    return word_counts, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PQwY3WkrPoT"
   },
   "source": [
    "#Compute Word Probabilities (with Smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gx1dmadApKNN"
   },
   "outputs": [],
   "source": [
    "def calculate_word_probs(k=1):\n",
    "    # TODO: For each word in vocab:\n",
    "    # P(word|spam) = (count in spam + k)/(total spam words + k*V)\n",
    "    # V = vocabulary size (len(vocab))\n",
    "    return word_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBaetzsrrbYj"
   },
   "source": [
    "#Predict Spam Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbCxkksUpT7t"
   },
   "outputs": [],
   "source": [
    "def predict(message, prior_prob, word_probs):\n",
    "    # TODO:\n",
    "    # 1. Preprocess message and split into words\n",
    "    # 2. Initialize log_prob_spam = log(P(spam))\n",
    "    # 3. For each word:\n",
    "    #    - if word in vocab: add log(P(word|spam))\n",
    "    # 4. Convert log probabilities back using exp()\n",
    "    # 5. Return P(spam|message) = spam_prob/(spam_prob + ham_prob)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bSCRCmWqPcn"
   },
   "outputs": [],
   "source": [
    "# Test calculate_priors()\n",
    "labels = ['spam', 'ham']\n",
    "print(calculate_priors(labels))  # Should return {'spam': 0.33, 'ham': 0.67}\n",
    "\n",
    "# Test count_words()\n",
    "messages = [\"free prize\", \"meeting today\"]\n",
    "word_counts, vocab = count_words(messages, labels)\n",
    "print(word_counts['spam']['free'])  # Should be 1\n",
    "print(len(vocab))  # Should be 3 (free, prize, meeting, today)\n",
    "\n",
    "# Test calculate_word_probs()\n",
    "word_probs = calculate_word_probs(k=1)\n",
    "print(word_probs['spam']['free'])  # Should be (1+1)/(2+1*4) = 0.33"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
